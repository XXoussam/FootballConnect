    # -*- coding: utf-8 -*-
"""tacticsense_player rating

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AFUb_WwOXL7_8uv6wU0codsqZiL8BZS0
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Scikit-learn imports
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.ensemble import GradientBoostingRegressor
import xgboost as xgb
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor

# Load dataset
df = pd.read_csv("player_rating_data_set.csv")

"""# Data understanding"""

# Display the first few rows of the dataset
df.head()

# Get a summary of the dataset, including the number of non-null entries
df.shape

# Display a table showing column names and their data types
column_info = pd.DataFrame({
    "Column Name": df.columns,
    "Data Type": df.dtypes.values
})

# Show only the first 50-60 columns for readability
column_info.head(60)

"""**Data distribution**

---


"""

# List of target columns to plot
target_columns = [ 'OVR', 'PAC', 'SHO', 'PAS', 'DRI', 'DEF', 'PHY']

# Create subplots for each column
fig, axes = plt.subplots(len(target_columns), 1, figsize=(12, 8 * len(target_columns)))  # Adjusting the size for each plot

# Plot histograms for each target column
for i, col in enumerate(target_columns):
    axes[i].hist(df[col], bins=20, color='green', edgecolor='black', alpha=0.7)
    axes[i].set_title(f'Histogram of {col}', fontsize=14)
    axes[i].set_xlabel(col, fontsize=12)
    axes[i].set_ylabel('Frequency', fontsize=12)

# Add a super title for all plots
plt.suptitle('Histograms of Target Features', fontsize=18, y=1.03)

# Adjust the layout
plt.tight_layout()
plt.show()

# Group the data by 'Age' and calculate the mean 'OVR' for each age group
average_ovr_by_age = df.groupby('Age')['OVR'].mean()

# Plot the average OVR for each age group
plt.figure(figsize=(12, 6))
average_ovr_by_age.plot(kind='line', color='skyblue', linewidth=2)
plt.title('Average Overall Rating (OVR) by Age')
plt.xlabel('Age')
plt.ylabel('Average OVR')
plt.grid(True)
plt.show()

# Calculate the correlation matrix
correlation_matrix = df[['Age', 'OVR', 'PAC', 'SHO', 'PAS', 'DRI', 'DEF', 'PHY']].corr()

# Set up the heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)

# Add a title
plt.title("Correlation Heatmap for Age and Performance Attributes")

# Show the plot
plt.show()

"""**This model will be applied to unestablished young players participating in scouting camps. Since the target distribution is clearly different for older players, we will focus our training on data for players under the age of 21. This approach ensures that the model is tailored specifically for the younger, developing talent.**

---


"""

# Filter the dataset to keep only U21 players
df_u21 = df[df['Age'] < 21].copy()

# Display the shape to confirm
print(f"Dataset filtered to U21 players: {df_u21.shape[0]} rows remaining.")

"""**Data distribution(U21)**

---


"""

# Plot the distribution of overall ratings (OVR) for U21 players without categorization
plt.figure(figsize=(10, 6))
plt.hist(df_u21['OVR'], bins=20, color='green', edgecolor='black')

# Add percentage labels above the bars
total_count = len(df_u21)
for i in range(len(plt.gca().patches)):
    height = plt.gca().patches[i].get_height()
    plt.text(plt.gca().patches[i].get_x() + plt.gca().patches[i].get_width() / 2, height + 0.5,
             f'{(height / total_count) * 100:.1f}%', ha='center', fontsize=10)

plt.title('Distribution of Overall Ratings (OVR) for U21 Players')
plt.xlabel('Overall Rating (OVR)')
plt.ylabel('Count')
plt.show()

"""**The overall ratings of U21 players are mostly concentrated in the average quality range, which reflects the data the model will use.**"""

# First, define the general categories for positions (GK, DEF, MID, ATT)
general_positions = {
    'GK': ['GK'],  # For simplicity, assuming players marked as GK are goalkeepers
    'DEF': ['CB', 'RB', 'LB', 'LWB', 'RWB', 'RCB', 'LCB'],  # All defender-related positions
    'MID': ['CM', 'CDM', 'CAM', 'RM', 'LM', 'RDM', 'LDM'],  # All midfielder-related positions
    'ATT': ['ST', 'CF', 'LW', 'RW', 'LF', 'RF']  # All attacker-related positions
}

# Function to categorize positions into general categories
def categorize_position(position):
    for general_pos, roles in general_positions.items():
        if position in roles:
            return general_pos
    return 'Other'

# Apply the function to categorize the positions
df_u21['General Position'] = df_u21['Position'].apply(categorize_position)

# Plot the general position distribution using a circle (pie) plot
plt.figure(figsize=(8, 8))
df_u21['General Position'].value_counts().plot(kind='pie', autopct='%1.1f%%', startangle=90, colors=plt.cm.Paired.colors)
plt.title('General Position Distribution for U21 Players')
plt.ylabel('')  # Hide the y-axis label for a cleaner look
plt.show()

"""**The pie chart shows a balanced distribution of positions among U21 players, providing a diverse representation of goalkeepers, defenders, midfielders, and attackers for a well-rounded analysis.**"""

# Group the data by 'Age' and calculate the mean 'OVR' for each age group
average_ovr_by_age = df_u21.groupby('Age')['OVR'].mean()

# Plot the average OVR for each age group
plt.figure(figsize=(12, 6))
average_ovr_by_age.plot(kind='line', color='skyblue', linewidth=2)
plt.title('Average Overall Rating (OVR) by Age')
plt.xlabel('Age')
plt.ylabel('Average OVR')
plt.grid(True)
plt.show()

"""**missing values**

---


"""

# Check for missing data
missing_data = df_u21.isnull()

# Create a heatmap to visualize the missing data
plt.figure(figsize=(12, 8))
sns.heatmap(missing_data, cmap='viridis', cbar=False, yticklabels=False, xticklabels=True)

# Set title and labels
plt.title('Missing Data Heatmap for U21 Players')
plt.xlabel('Columns')
plt.ylabel('Rows')

plt.show()

"""**The missing values in goalkeeper-specific features suggest that goalkeepers should be treated as a separate category. Their unique attributes, such as GK Diving and GK Reflexes, are not relevant for outfield players, and vice versa. This separation allows for more accurate training by building specialized models for goalkeepers and outfield players, ensuring each group is trained on the relevant features for better performance.**

**outliers**

---
"""

# Define a function to detect outliers using the IQR method
def detect_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    return outliers

# Detect outliers for relevant columns
outliers_ovr = detect_outliers(df_u21, 'OVR')
outliers_pac = detect_outliers(df_u21, 'PAC')
outliers_sho = detect_outliers(df_u21, 'SHO')
outliers_pas = detect_outliers(df_u21, 'PAS')
outliers_dri = detect_outliers(df_u21, 'DRI')
outliers_def = detect_outliers(df_u21, 'DEF')
outliers_phy = detect_outliers(df_u21, 'PHY')

# Combine all outliers into a single DataFrame
outliers_all = pd.concat([outliers_ovr, outliers_pac, outliers_sho, outliers_pas, outliers_dri, outliers_def, outliers_phy]).drop_duplicates()

# Show the outliers
outliers_all

"""**The outlier analysis reveals that there aren't many outliers in the dataset. However, these outliers are important as they represent players who either perform exceptionally well or poorly. These extreme values are crucial for our model, as they highlight both exceptional talents and those who may require further development. Instead of removing these outliers, we will retain them, as they provide valuable insights for training the model to handle a wider range of player performances.**

**Since the distribution of ratings and positions is fairly balanced, advanced techniques like SMOTE or other data filling methods are unnecessary. This ensures the model trains effectively without dealing with significant bias or imbalance.**

# Data Preparation

**filtering u21 players and sepearting goalkeepers from outfield players**





---
"""

# Filter only U21 players
df_u21 = df[df['Age'] < 21].copy()

# Separate goalkeepers (GK) from other players
df_gk = df_u21[df_u21['Position'] == 'GK'].reset_index(drop=True)
df_players = df_u21[df_u21['Position'] != 'GK'].reset_index(drop=True)

"""**Feature Engineering**

---


"""

# Drop goalkeeper-specific columns from the outfield players dataset
gk_columns = ['GK Diving', 'GK Handling', 'GK Kicking', 'GK Positioning', 'GK Reflexes']
df_players = df_players.drop(columns=gk_columns)

# Remove the 'AlternativePosition' column from the goalkeepers dataset
df_gk = df_gk.drop(columns=['Alternative positions'])

df_players.columns = df_players.columns.str.strip()
df_gk.columns = df_gk.columns.str.strip()

# Remove irrelevant columns
df_players = df_players.drop(columns=['Name', 'Unnamed: 0'])

# Remove irrelevant columns in the GK dataset
df_gk = df_gk.drop(columns=['Name','Unnamed: 0'])

"""**handling missing values**





---


"""

# Plot missing values heatmap for players dataset
plt.figure(figsize=(12, 6))
sns.heatmap(df_players.isnull(), cbar=False, cmap='viridis')
plt.title("Missing Values in Players Dataset")
plt.show()

# Plot missing values heatmap for goalkeepers dataset
plt.figure(figsize=(12, 6))
sns.heatmap(df_gk.isnull(), cbar=False, cmap='viridis')
plt.title("Missing Values in Goalkeepers Dataset")
plt.show()

# For players dataset
df_players['Alternative positions'] = df_players['Alternative positions'].fillna('No Alternative Position')
df_players['play style'] = df_players['play style'].fillna('Unknown')

# For goalkeepers dataset
df_gk['play style'] = df_gk['play style'].fillna('Unknown')

"""**Columns with high missing values are usually dropped. However, 'play style' can be a strong indicator of key attributes (e.g., dribbling play styles often signal high dribbling ratings), so it is retained as a potentially important feature.**"""

# Heatmap for missing values in the players dataset
plt.figure(figsize=(15, 10))
sns.heatmap(df_players.isnull(), cbar=False, cmap='viridis', yticklabels=False)
plt.title("Missing Values Heatmap - Players")
plt.show()

# Heatmap for missing values in the goalkeepers dataset
plt.figure(figsize=(15, 10))
sns.heatmap(df_gk.isnull(), cbar=False, cmap='viridis', yticklabels=False)
plt.title("Missing Values Heatmap - Goalkeepers")
plt.show()

"""**handling outleirs**

---

outliers will be retained , as they provide valuable insights for training the model to handle a wider range of player performances.

**Categorical Encoding**

---
"""

# Identify categorical columns in both datasets
categorical_columns_players = df_players.select_dtypes(include=['object']).columns
categorical_columns_gk = df_gk.select_dtypes(include=['object']).columns

# Show the categorical columns
print("Categorical columns in the players dataset:")
print(categorical_columns_players)

print("\nCategorical columns in the goalkeepers dataset:")
print(categorical_columns_gk)

# Show the first few rows of the dataset to inspect the columns
print(df_players[['Height', 'Weight']].head())

"""**weight and height**

---


"""

import re

def clean_height_weight(df):
    def extract_height_weight(value, unit):
        """Extract integer values for height (cm) and weight (kg), handling typos."""
        if not isinstance(value, str):  # Ensure value is a string
            return None
        value = value.lower().strip()
        match = re.search(r"(\d+)\s*" + unit, value)  # Match number followed by cm or kg
        if match:
            return int(match.group(1))
        return None  # Return None if no valid number found

    # Apply extraction to Height and Weight columns
    df['Height'] = df['Height'].astype(str).apply(lambda x: extract_height_weight(x, 'cm'))
    df['Weight'] = df['Weight'].astype(str).apply(lambda x: extract_height_weight(x, 'kg'))

    # Check for any rows with missing (invalid) values after conversion
    invalid_rows = df[df[['Height', 'Weight']].isnull().any(axis=1)]
    if not invalid_rows.empty:
        print(f"\nRows with invalid Height or Weight values in dataset:")
        print(invalid_rows[['Height', 'Weight']])

    return df

# Apply cleaning function to both datasets
df_players = clean_height_weight(df_players)
df_gk = clean_height_weight(df_gk)

# Show final processed data
print("\nFinal Processed Data (Players):")
print(df_players[['Height', 'Weight']].head())

print("\nFinal Processed Data (Goalkeepers):")
print(df_gk[['Height', 'Weight']].head())

# Show the first few rows of the dataset to inspect the columns
print(df_players[['Height', 'Weight']].head())

"""**Play styles**

---


"""

max_play_styles = df['play style'].apply(lambda x: len(str(x).split(',')) if pd.notna(x) else 0).max()
print(f"Maximum number of play styles in the dataset: {max_play_styles}")

def get_unique_play_styles(df, column_name='play style', position_column='Position'):

    unique_styles_players = set()
    unique_styles_gk = set()

    for index, row in df.dropna(subset=[column_name]).iterrows():
        play_styles = [s.strip() for s in row[column_name].split(',')]

        # Check if the position contains "GK" (Goalkeeper)
        if isinstance(row[position_column], str) and "GK" in row[position_column]:
            unique_styles_gk.update(play_styles)
        else:
            unique_styles_players.update(play_styles)

    return unique_styles_players, unique_styles_gk

# Example usage
unique_styles_players, unique_styles_gk = get_unique_play_styles(df)

print("Unique Play Styles for Players:", unique_styles_players)
print("Unique Play Styles for Goalkeepers:", unique_styles_gk)

# Print column names of df_players to verify
print("Columns in df_players:", df_players.columns)

# Define the playstyle groups
play_style_groups = {
    'pace_ps': ['Rapid', 'Rapid+', 'Quick Step', 'Quick Step+'],
    'phy_ps': ['Bruiser', 'Bruiser+', 'Aerial', 'Aerial+', 'Relentless', 'Relentless+'],
    'defending_ps': ['Block', 'Block+', 'Intercept', 'Intercept+', 'Jockey', 'Jockey+', 'Slide Tackle',
                      'Slide Tackle+', 'Anticipate', 'Anticipate+', 'Press Proven', 'Press Proven+'],
    'passing_ps': ['Pinged Pass', 'Pinged Pass+', 'Incisive Pass', 'Incisive Pass+', 'Whipped Pass',
                    'Whipped Pass+', 'Long Ball Pass', 'Long Ball Pass+', 'Tiki Taka', 'Tiki Taka+',
                    'Long Throw', 'Long Throw+'],
    'shooting_ps': ['Power Shot', 'Power Shot+', 'Finesse Shot', 'Finesse Shot+', 'Chip Shot', 'Chip Shot+',
                    'Trivela', 'Trivela+', 'Power Header', 'Power Header+'],
    'dribbling_ps': ['First Touch', 'First Touch+', 'Flair', 'Flair+', 'Trickster', 'Trickster+', 'Technical',
                     'Technical+', 'Acrobatic', 'Acrobatic+', 'Dead Ball', 'Dead Ball+']
}

def encode_play_styles(df, column_name='play style'):
    """
    Encodes play styles by counting the number of play styles in each group.
    Play styles ending with '+' are considered with a value of 2 (for excellence).

    Parameters:
    df (pd.DataFrame): The DataFrame containing player data.
    column_name (str): The column name containing play styles.

    Returns:
    pd.DataFrame: A DataFrame with the encoded play style group columns.
    """
    # Initialize empty columns for each group
    for group in play_style_groups:
        df[group] = 0

    # Iterate over each row in the DataFrame
    for idx, row in df.iterrows():
        # Get the player's play styles (split by commas and strip any extra spaces)
        styles = row[column_name]
        if pd.notna(styles):
            style_list = [s.strip() for s in styles.split(',')]

            # Count how many play styles belong to each group and consider '+' as 2
            for group, group_styles in play_style_groups.items():
                count = 0
                for style in style_list:
                    if style in group_styles:
                        if style.endswith('+'):  # If style ends with '+', count as 2
                            count += 2
                        else:  # Otherwise, count as 1
                            count += 1
                df.at[idx, group] = count  # Set the count for each group

    return df

# Example usage
df_encoded = encode_play_styles(df_players, column_name='play style')

# Display the encoded DataFrame (showing first few columns for a sample)
df_encoded[['Rank', 'pace_ps', 'phy_ps', 'defending_ps', 'passing_ps', 'shooting_ps', 'dribbling_ps']].head()

# Define the goalkeeper playstyle groups
gk_play_style_groups = {
    'handling_ps': ['Cross Claimer', 'Cross Claimer+', '1v1 Close Down', '1v1 Close Down+'],
    'kicking_ps': ['Far Throw', 'Far Throw+', 'Long Ball Pass', 'Long Ball Pass+'],
    'positioning_ps': ['Far Reach', 'Far Reach+', 'Deflector', 'Deflector+'],
    'footwork_ps': ['Footwork', 'Footwork+']
}

def encode_gk_play_styles(df, column_name='play style'):
    """
    Encodes goalkeeper play styles by counting the number of play styles in each group.
    Play styles ending with '+' are considered with a value of 2 (for excellence).

    Parameters:
    df (pd.DataFrame): The DataFrame containing goalkeeper data.
    column_name (str): The column name containing play styles.

    Returns:
    pd.DataFrame: A DataFrame with the encoded play style group columns.
    """
    # Initialize empty columns for each group
    for group in gk_play_style_groups:
        df[group] = 0

    # Iterate over each row in the DataFrame
    for idx, row in df.iterrows():
        # Get the player's play styles (split by commas and strip any extra spaces)
        styles = row[column_name]
        if pd.notna(styles):
            style_list = [s.strip() for s in styles.split(',')]

            # Count how many play styles belong to each group and consider '+' as 2
            for group, group_styles in gk_play_style_groups.items():
                count = 0
                for style in style_list:
                    if style in group_styles:
                        if style.endswith('+'):  # If style ends with '+', count as 2
                            count += 2
                        else:  # Otherwise, count as 1
                            count += 1
                df.at[idx, group] = count  # Set the count for each group

    return df

# Example usage
df_gk_encoded = encode_gk_play_styles(df_gk, column_name='play style')

# Display the encoded DataFrame (showing first few columns for a sample)
df_gk_encoded[['Rank', 'handling_ps', 'kicking_ps', 'positioning_ps', 'footwork_ps']].head()

"""**We chose counting and grouping over one-hot encoding, as one-hot would create too many columns, leading to a sparse matrix. Label encoding doesn't work well here since players can have up to 7 play styles, making it unsuitable for capturing the full range of abilities. This approach provides a more compact and meaningful representation of the data, preserving the skill diversity without unnecessary complexity.**"""

# Drop the 'play style' column from both dataframes
df_players.drop(columns=['play style'], inplace=True)
df_gk.drop(columns=['play style'], inplace=True)

# Get the categorical columns after dropping 'play style'
categorical_columns_players = df_players.select_dtypes(include=['object']).columns.tolist()
categorical_columns_gk = df_gk.select_dtypes(include=['object']).columns.tolist()

# Return the categorical columns
categorical_columns_players, categorical_columns_gk

"""**label encoding to the rest of the categorical columns**

---


"""

# Initialize label encoder
label_encoder = LabelEncoder()

# Apply label encoding to the categorical columns for df_players
for column in categorical_columns_players:
    df_players[column] = label_encoder.fit_transform(df_players[column])

# Apply label encoding to the categorical columns for df_gk
for column in categorical_columns_gk:
    df_gk[column] = label_encoder.fit_transform(df_gk[column])

# Verify the encoding (checking the first few rows of both DataFrames)
df_players.head(), df_gk.head()

"""**standarization**

---


"""

# Initialize the standard scaler
scaler = StandardScaler()

# List of numerical columns for df_players
numerical_columns_players = df_players.select_dtypes(include=['int64', 'float64']).columns

# List of numerical columns for df_gk
numerical_columns_gk = df_gk.select_dtypes(include=['int64', 'float64']).columns

# Apply standardization to df_players
df_players[numerical_columns_players] = scaler.fit_transform(df_players[numerical_columns_players])

# Apply standardization to df_gk
df_gk[numerical_columns_gk] = scaler.fit_transform(df_gk[numerical_columns_gk])

# Verify the standardization (checking the first few rows of both DataFrames)
df_players.head(), df_gk.head()

"""**Splitting Data**

---


"""

# Splitting df_players into separate DataFrames for each attribute
pac = df_players[['PAC'] + [col for col in df_players.columns if col != 'PAC']]
sho = df_players[['SHO'] + [col for col in df_players.columns if col != 'SHO']]
pas = df_players[['PAS'] + [col for col in df_players.columns if col != 'PAS']]
dri = df_players[['DRI'] + [col for col in df_players.columns if col != 'DRI']]
def_ = df_players[['DEF'] + [col for col in df_players.columns if col != 'DEF']]
phy = df_players[['PHY'] + [col for col in df_players.columns if col != 'PHY']]
ovr = df_players[['OVR'] + [col for col in df_players.columns if col != 'OVR']]

# For df_gk, the split will only be for OVR since it's the only target
gk_ovr = df_gk[['OVR'] + [col for col in df_gk.columns if col != 'OVR']]

# Output the new DataFrames for verification
print("DataFrames for df_players:")
print("PAC DataFrame:", pac.shape)
print("SHO DataFrame:", sho.shape)
print("PAS DataFrame:", pas.shape)
print("DRI DataFrame:", dri.shape)
print("DEF DataFrame:", def_.shape)
print("PHY DataFrame:", phy.shape)
print("OVR DataFrame:", ovr.shape)

print("\nDataFrame for df_gk:")
print("OVR DataFrame:", gk_ovr.shape)

# For df_players:
X_pac = pac.drop(columns=['PAC'])
y_pac = pac['PAC']

X_sho = sho.drop(columns=['SHO'])
y_sho = sho['SHO']

X_pas = pas.drop(columns=['PAS'])
y_pas = pas['PAS']

X_dri = dri.drop(columns=['DRI'])
y_dri = dri['DRI']

X_def = def_.drop(columns=['DEF'])
y_def = def_['DEF']

X_phy = phy.drop(columns=['PHY'])
y_phy = phy['PHY']

X_ovr = ovr.drop(columns=['OVR'])
y_ovr = ovr['OVR']

# For df_gk (since it's only predicting OVR):
X_gk = gk_ovr.drop(columns=['OVR'])
y_gk = gk_ovr['OVR']

# Train-test split for each target (80% training, 20% testing)
X_train_pac, X_test_pac, y_train_pac, y_test_pac = train_test_split(X_pac, y_pac, test_size=0.2, random_state=42)
X_train_sho, X_test_sho, y_train_sho, y_test_sho = train_test_split(X_sho, y_sho, test_size=0.2, random_state=42)
X_train_pas, X_test_pas, y_train_pas, y_test_pas = train_test_split(X_pas, y_pas, test_size=0.2, random_state=42)
X_train_dri, X_test_dri, y_train_dri, y_test_dri = train_test_split(X_dri, y_dri, test_size=0.2, random_state=42)
X_train_def, X_test_def, y_train_def, y_test_def = train_test_split(X_def, y_def, test_size=0.2, random_state=42)
X_train_phy, X_test_phy, y_train_phy, y_test_phy = train_test_split(X_phy, y_phy, test_size=0.2, random_state=42)
X_train_ovr, X_test_ovr, y_train_ovr, y_test_ovr = train_test_split(X_ovr, y_ovr, test_size=0.2, random_state=42)

# For df_gk (OVR):
X_train_gk, X_test_gk, y_train_gk, y_test_gk = train_test_split(X_gk, y_gk, test_size=0.2, random_state=42)

# Print shapes to verify
print(f"Training and Testing split for PAC: X_train={X_train_pac.shape}, X_test={X_test_pac.shape}")
print(f"Training and Testing split for SHO: X_train={X_train_sho.shape}, X_test={X_test_sho.shape}")
print(f"Training and Testing split for PAS: X_train={X_train_pas.shape}, X_test={X_test_pas.shape}")
print(f"Training and Testing split for DRI: X_train={X_train_dri.shape}, X_test={X_test_dri.shape}")
print(f"Training and Testing split for DEF: X_train={X_train_def.shape}, X_test={X_test_def.shape}")
print(f"Training and Testing split for PHY: X_train={X_train_phy.shape}, X_test={X_test_phy.shape}")
print(f"Training and Testing split for OVR: X_train={X_train_ovr.shape}, X_test={X_test_ovr.shape}")

print(f"\nTraining and Testing split for GK OVR: X_train={X_train_gk.shape}, X_test={X_test_gk.shape}")

"""# Modeling

**Random Forest**

---
"""

# Define a reduced grid of hyperparameters
param_grid = {
    'n_estimators': [50, 100],  # Number of trees in the forest
    'max_depth': [10, 20],  # Maximum depth of the trees
    'min_samples_split': [2, 5],  # Minimum samples to split a node
    'min_samples_leaf': [1, 2],  # Minimum samples per leaf node
    'max_features': ['sqrt', 'log2']  # Feature selection strategy

# Initialize the Random Forest model
rf = RandomForestRegressor(random_state=42)

# GridSearchCV for each target
grid_search_rf_pac = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5,
                                  scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)
grid_search_rf_sho = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5,
                                  scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)
grid_search_rf_pas = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5,
                                  scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)
grid_search_rf_dri = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5,
                                  scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)
grid_search_rf_def = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5,
                                  scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)
grid_search_rf_phy = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5,
                                  scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)
grid_search_rf_ovr_players = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5,
                                          scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)
grid_search_rf_ovr_gk = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5,
                                     scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)

# Fit models for players' targets
grid_search_rf_pac.fit(X_train_pac, y_train_pac)
grid_search_rf_sho.fit(X_train_sho, y_train_sho)
grid_search_rf_pas.fit(X_train_pas, y_train_pas)
grid_search_rf_dri.fit(X_train_dri, y_train_dri)
grid_search_rf_def.fit(X_train_def, y_train_def)
grid_search_rf_phy.fit(X_train_phy, y_train_phy)

# Fit model for players' OVR target
grid_search_rf_ovr_players.fit(X_train_ovr, y_train_ovr)

# Fit model for goalkeepers' OVR target
grid_search_rf_ovr_gk.fit(X_train_gk, y_train_gk)